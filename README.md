# My Machine Learning Notes

## Pretrained Models
|Model description|Code|Paper
|---|---|---|
|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|<a href="https://github.com/google-research/bert">code</a>|<a href="https://arxiv.org/abs/1810.04805">paper</a>|
|Flair Embeddings|<a href="https://github.com/zalandoresearch/flair">code</a>|<a href="https://drive.google.com/file/d/17yVpFA7MmXaQFTe-HDpZuqw9fJlmzg56/view?usp=sharing">paper</a>|
|Generative Pre-Training (GPT-2)|<a href="https://github.com/openai/gpt-2">code</a>|<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">paper</a>|
|ELMo: Deep contextualized word representations|<a href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md">code</a>|<a href="http://www.aclweb.org/anthology/N18-1202">paper</a>|
||<a href="">code</a>|<a href="">paper</a>|
